# Chrome Prompt API Integration

## √úberblick

GMARK nutzt die **Chrome Prompt API** f√ºr lokale, On-Device LLM-basierte Klassifikation von Webseiten. Das Modell (Gemini Nano) l√§uft direkt im Browser ohne externe API-Aufrufe.

## Klassifikations-Hierarchie

Die Extension nutzt folgende **Fallback-Strategie**:

1. **Chrome Prompt API (Priorit√§t: H√ñCHST)** ‚ö°
   - Lokale On-Device LLM (Gemini Nano)
   - ‚úÖ Keine API-Abh√§ngigkeit
   - ‚úÖ Schnellste Klassifikation
   - ‚úÖ Datenschutz: Daten bleiben lokal
   - ‚ö†Ô∏è Nur auf Chrome 131+ mit aktiviertem Flag verf√ºgbar
   - ‚ö†Ô∏è Erstes Download des Modells kann 100MB+ sein

2. **Backend Classification API (Fallback)** üåê
   - Pattern-basierte Klassifikation
   - ‚úÖ Funktioniert offline nach Download
   - ‚úÖ Zuverl√§ssig auf allen Systemen
   - ‚úÖ Keine Hardware-Anforderungen

3. **Offline Pattern Matching (Ultimate Fallback)** üìä
   - Lokale Keyword-Analyse
   - ‚úÖ Funktioniert immer
   - ‚úÖ Keine Netzwerkverbindung erforderlich

## Verf√ºgbarkeit pr√ºfen

```javascript
// Pr√ºfe ob Prompt API verf√ºgbar ist
const canUse = await window.ai.canCreateTextSession();

// M√∂gliche Werte:
// "readily"       ‚Üí sofort nutzbar
// "after-download" ‚Üí nach Modell-Download verf√ºgbar
// "no"            ‚Üí nicht verf√ºgbar (kein Chrome 131+, kein Flag)
```

## Aktivierung (Chrome Browser)

### Voraussetzungen:
- Chrome 131+ (die API wird schrittweise eingef√ºhrt)
- 4GB+ RAM (f√ºr Modell-Inferenz)
- ~100MB Speicher (Gemini Nano Modell)

### Schritte:

1. **Chrome Flags aktivieren:**
   ```
   chrome://flags/#prompt-api-for-gemini-nano
   ```
   ‚Üí Auf "Enabled" setzen

2. **Chrome Neustart:**
   ```
   chrome://restart
   ```

3. **Extension laden:**
   - Gehe zu `chrome://extensions`
   - Schalte "Entwicklermodus" an
   - "Erweiterung laden" ‚Üí `/browser-extension` Ordner

4. **Popup √∂ffnen:**
   - Klicke auf Extension-Icon
   - Aktiviere "Mit AI klassifizieren" Checkbox
   - Warte auf Modell-Download (beim ersten Mal)

## Klassifikations-Kategorien

Die Prompt API klassifiziert URLs in folgende Kategorien:

```
- Development     (Code, GitHub, APIs, Frameworks)
- Social         (Twitter, Facebook, Instagram, LinkedIn)
- News           (Nachrichten, Blogs, Artikel)
- Shopping       (Amazon, eBay, Shops, Produkte)
- Education      (Kurse, Tutorials, Universit√§ten)
- Entertainment  (Netflix, Filme, Musik, Spiele)
- Documentation  (Technische Docs, Handb√ºcher)
- Tools          (Online-Tools, Converter, Generatoren)
- Other          (Sonstiges)
```

## Performance

### Erste Nutzung (mit Modell-Download):
- Download: ~100MB (einmalig)
- Klassifikation: 2-5 Sekunden
- Extension zeigt: "‚è≥ Laden Sie das lokale LLM-Modell..."

### Nachfolgende Nutzung (Modell gecacht):
- Klassifikation: 200-800ms
- ‚ö° Ultra-schnell, offline verf√ºgbar

## Datenschutz

‚úÖ **Vollst√§ndiger Datenschutz mit Prompt API:**
- Keine Daten verlassen den Browser
- Keine Requests zu externen Servern
- Lokale Verarbeitung auf deinem Ger√§t
- Modell wird nur f√ºr Klassifikation verwendet
- Keine Logs oder Telemetrie

‚ö†Ô∏è **Backend-Fallback:**
- Bei Fallback zum Backend werden URL + Title gesendet
- Aber: nur wenn Prompt API nicht verf√ºgbar ist
- Du kannst Backend-Klassifikation in Einstellungen deaktivieren

## Troubleshooting

### Problem: "Prompt API nicht verf√ºgbar"
**L√∂sung:**
1. Chrome Version pr√ºfen: `chrome://version` ‚Üí mind. 131
2. Flag aktivieren: `chrome://flags/#prompt-api-for-gemini-nano`
3. Chrome neustarten: `chrome://restart`

### Problem: Klassifikation dauert lange (erste Nutzung)
**L√∂sung:**
- Das ist normal beim ersten Download des Modells
- Modell wird danach gecacht, zuk√ºnftige Klassifikationen sind schneller
- Download l√§uft im Hintergrund

### Problem: "Classification fehlgeschlagen"
**L√∂sung:**
1. Fallback zu Backend wird automatisch versucht
2. Pr√ºfe Internet-Verbindung
3. Pr√ºfe ob Backend-Server l√§uft (auf Port 8000)

## Umschaltung zwischen Methoden

In der Extension (popup.js):

```javascript
// Priorisierung anpassen:
const result = await classifyWithPromptAPI(url, title);
// ‚Üí Bei null: automatisch Fallback zu Backend

// Backend-Klassifikation deaktivieren (nur lokal):
// Entferne Fallback in classifyPage() Funktion
```

## Entwicklung & Testing

### Prompt API lokal testen:

```javascript
// In Browser Console √∂ffnen (F12):
await window.ai.canCreateTextSession()

// Wenn "readily" oder "after-download" ‚Üí Verf√ºgbar!

// Session erstellen und Prompt senden:
const session = await window.ai.createTextSession();
const response = await session.prompt("Classify: GitHub");
console.log(response);
await session.destroy();
```

### Debug-Logs in Extension:

```
F12 ‚Üí Service Worker Console
‚Üí Sehe Klassifikations-Logs mit Methode (prompt-api vs backend)
```

## Zukunft

**Geplant:**
- [ ] Speichern von Klassifikations-Historie
- [ ] Modell-Optimierung f√ºr Bookmark-Kategorien
- [ ] Lokale Model Fine-Tuning basierend auf User-Feedback
- [ ] Unterst√ºtzung f√ºr weitere Chrome Built-in APIs (LanguageDetection, etc.)

## Referenzen

- [Chrome Prompt API Docs](https://developer.chrome.com/docs/ai/built-in-apis?hl=de#prompt_api)
- [Gemini Nano Model Info](https://ai.google.dev/gemini-api/docs/models/gemini)
- [Chrome Origin Trial](https://developer.chrome.com/origintrials/#/view_trial/3821286622385168385)
